{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MapScore.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNi9c7h+Oviz3JEUOkFIjw7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1F6aRr_PMfa"
      },
      "source": [
        "\"\"\"\n",
        "Use MapScore.score(<predicted>, <actual>) to get the MAP score over the entire testing dataset.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQUDUWVpGrXe"
      },
      "source": [
        "\"\"\"\n",
        "A function to calcualte the precision@k.\n",
        "\n",
        "Input: Two lists and a number.\n",
        "      - 'predicted' is the list of file names that our algorithm generates in response to a specific query\n",
        "      - 'actual' is the list of file names that our AI algorithm is supposed to return\n",
        "      - 'k' is the k-index for which we're supposed to calculate the precision@k\n",
        "\n",
        "Output: A number denoting the precision@k\n",
        "\"\"\"\n",
        "\n",
        "def precision_at_k(predicted, actual, k):\n",
        "    act_set = set(actual)\n",
        "    pred_set = set(predicted[:k])\n",
        "    result = len(act_set & pred_set) / float(k)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xsqWPBxEdPt"
      },
      "source": [
        "\"\"\"\n",
        "A function to calculate the average precision for a specific query.\n",
        "\n",
        "Input: Two lists.\n",
        "      - 'predicted' is the list of file names that our algorithm generates in response to a specific query\n",
        "      - 'actual' is the list of file names that our AI algorithm is supposed to return\n",
        "\n",
        "Output: A number denoting the average precision for a query.\n",
        "\n",
        "Things to check for: If the length of our predicted array is less than the length of our actual array, the code will fail (ideally this shouldn't happen, and should be checked for before calling the map score function)\n",
        "\"\"\"\n",
        "\n",
        "def avg_precision(predicted, actual):\n",
        "  avg_prec = 0\n",
        "  n = 0\n",
        "\n",
        "  for i in range(len(actual)):\n",
        "    if predicted[i] == actual[i]:\n",
        "      avg_prec += precision_at_k(predicted, actual, i+1):\n",
        "      n += 1\n",
        "\n",
        "  avg_prec /= n\n",
        "  return avg_prec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gejh30XhCYUU"
      },
      "source": [
        "\"\"\"\n",
        "A function to calculate the Mean Average Precision (MAP) Score for the entire testing dataset.\n",
        "\n",
        "Input: Two 2D Lists. \n",
        "      - 'predicted' is the list of list of file names that our algorithm generates. Each list corresponds to one input\n",
        "      - 'actual' is the list of list of file names that we're supposed to get. Each list corresponds to one input\n",
        "\n",
        "Output: A number denoting the map_score\n",
        "\"\"\"\n",
        "\n",
        "def score(predicted, actual):\n",
        "  map_score = 0\n",
        "  n = 0\n",
        "\n",
        "  for i in range(len(actual)):\n",
        "    map_score += get_avg_precision(predicted[i], actual[i])\n",
        "    n += 1\n",
        "\n",
        "  map_score /= n\n",
        "  return map_score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}