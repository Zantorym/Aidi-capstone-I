{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDI1003_Capstone_Dataset_Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zantorym/Aidi-capstone-I/blob/review/AIDI1003_Capstone_Dataset_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDs9RqdgLOEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab10717e-5bbb-40e1-a3cc-958c5a662015"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 173 Âµs (started: 2021-11-04 18:07:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2p0FHxT_l31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7304c13c-5d84-4875-9e83-59d426a90431"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.util import ngrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.41 s (started: 2021-11-04 18:07:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyA4cecCLXBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72f84de-7b69-4b1c-b7e9-1fc4d7b56c7d"
      },
      "source": [
        "# Constants\n",
        "# Pickle Input\n",
        "JD_FILES_PICKLE_OUTPATH='/content/drive/MyDrive/Durham College/Capstone - I/data/Datasets/jds.pickle'\n",
        "RESUME_FILES_PICKLE_OUTPATH='/content/drive/MyDrive/Durham College/Capstone - I/data/Datasets/resumes.pickle'\n",
        "\n",
        "# Tokenization\n",
        "'''\n",
        "0 - string split\n",
        "1 - NLTK TreebankWordTokenizer\n",
        "Note: not defined value = 0 = string split\n",
        "'''\n",
        "TOKENIZATION_ALGORITHM=0\n",
        "\n",
        "\n",
        "# NGrams\n",
        "NGRAM_COUNT=2\n",
        "\n",
        "# Stop Words\n",
        "FILTER_STOP_WORDS=1\n",
        "STOP_WORDS_SOURCE=0\n",
        "'''\n",
        "1 - Use NLTK stop words\n",
        "2 - Use Scikit Learn stop words\n",
        "Note: not defined value = 0 = intersection of both NLTK and Scikit-learn\n",
        "'''\n",
        "\n",
        "# Case Folding\n",
        "# Note: case folding is always performed as job description and resumes\n",
        "#       should have minimal use of proper nouns for differentiating against \n",
        "#       common words.\n",
        "\n",
        "# Stemming\n",
        "STEMMER_ALGORITHM=0\n",
        "'''\n",
        "1 = Use Porter stemmer\n",
        "2 = Use Snowball stemmer\n",
        "Note: not defined value = 0 = no stemming performed\n",
        "'''\n",
        "\n",
        "# Lemmatization\n",
        "# Note: Cannot perform lematization as punctuation is removed from source text.\n",
        "#       Lemmatization requires parts of speech to work properly.\n",
        "\n",
        "# Filtering non-alphabetic tokens\n",
        "FILTER_NON_ALPHABETIC_TOKENS = 1\n",
        "'''\n",
        "0 = Don't filter\n",
        "1 = Filter\n",
        "'''\n",
        "\n",
        "# Pickle Output\n",
        "JD_TOKENS_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/JDs/jds-tokenized.pickle'\n",
        "RESUME_TOKENS_PICKLE_OUTPATH='/content/drive/MyDrive/Durham College/Capstone - I/data/Datasets/resumes-tokenized.pickle'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.2 ms (started: 2021-11-04 18:11:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fueE2I5YOM-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a689e1d1-eb5d-4c5b-f6e3-6e33ca01cef7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ9orrgWMRlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95e12bb-2493-426f-9436-64e9b135daa2"
      },
      "source": [
        "jd_files_dict = resume_files_dict = {}\n",
        "with open(JD_FILES_PICKLE_OUTPATH, 'rb') as fh:\n",
        "  jd_files_dict = pickle.load(fh)\n",
        "with open(RESUME_FILES_PICKLE_OUTPATH, 'rb') as fh:\n",
        "  resume_files_dict = pickle.load(fh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.8 s (started: 2021-11-04 18:07:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26EwFigkFiD",
        "outputId": "24fefbd8-ac32-4b3e-b102-d87518b16e24"
      },
      "source": [
        "if FILTER_STOP_WORDS == 1:\n",
        "  if STOP_WORDS_SOURCE != 2:\n",
        "    nltk.download('stopwords')\n",
        "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
        "  \n",
        "  if STOP_WORDS_SOURCE != 1:\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
        "\n",
        "  if STOP_WORDS_SOURCE == 1:\n",
        "    stop_words = nltk_stop_words\n",
        "  elif STOP_WORDS_SOURCE == 2:\n",
        "    stop_words = sklearn_stop_words\n",
        "  else:\n",
        "    stop_words = sklearn_stop_words.intersection(nltk_stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "time: 62.3 ms (started: 2021-11-04 18:07:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH-g6ZJ-5sEH",
        "outputId": "648f2259-abbf-4c02-e0e0-371d8e6cb1e7"
      },
      "source": [
        "if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "  from nltk.stem.snowball import SnowballStemmer\n",
        "  if STEMMER_ALGORITHM == 1:\n",
        "    stemmer = SnowballStemmer(language='porter')\n",
        "  elif STEMMER_ALGORITHM == 2:\n",
        "    stemmer = SnowballStemmer(language='english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.3 ms (started: 2021-11-04 18:07:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_vpmrw2Fa_I"
      },
      "source": [
        "'''\n",
        "Filters out non-alphabetic strings from a list of strings\n",
        "\n",
        "args:\n",
        "  * words - A list of strings\n",
        "\n",
        "Returns:\n",
        "  * A filtered list of strings\n",
        "'''\n",
        "def filter_alpha_space(words):\n",
        "  fil = []\n",
        "  for string in df:\n",
        "    if (any(x.isalpha() for x in string) and all(x.isalpha() or x.isspace() for x in string)):\n",
        "      fil.append(string) \n",
        "  return fil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aau7f6Wd3HCS",
        "outputId": "1890380d-ecfb-4829-a288-918daac97800"
      },
      "source": [
        "# Tokenize Resumes\n",
        "resumes_tokenized = {}\n",
        "for key in resume_files_dict:\n",
        "  tokenized = []\n",
        "\n",
        "  # Tokenize words\n",
        "  if TOKENIZATION_ALGORITHM == 1:\n",
        "    tokenized = TreebankWordTokenizer().tokenize(resume_files_dict[key].lower())\n",
        "  else:\n",
        "    tokenized = resume_files_dict[key].lower().split()\n",
        "\n",
        "  if FILTER_STOP_WORDS == 1:\n",
        "    tokenized = [token for token in tokenized if (token not in stop_words and len(token)>2)]\n",
        "\n",
        "  if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "    tokenized = [stemmer.stem(token) for token in tokenized]\n",
        "\n",
        "  # Tokenize ngrams\n",
        "  if NGRAM_COUNT > 1:\n",
        "    # Handle files that are \"empty\", i.e. contains only spaces\n",
        "    if len(tokenized) == 0:\n",
        "      ngram_tokens = []\n",
        "    else:\n",
        "      ngram_tokens = [' '.join(t) for t in ngrams(tokenized, NGRAM_COUNT)]\n",
        "    tokenized += ngram_tokens\n",
        "\n",
        "  if FILTER_NON_ALPHABETIC_TOKENS == 1:\n",
        "    tokenized = filter_alpha_space(tokenized)\n",
        "\n",
        "  resumes_tokenized[key] = tokenized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3min 24s (started: 2021-11-04 18:07:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3axQ4ALnS6N",
        "outputId": "9276fba0-3dfa-4c74-bd18-d99f254245c7"
      },
      "source": [
        "# Tokenize JDs\n",
        "jds_tokenized = {}\n",
        "for key in jd_files_dict:\n",
        "  tokenized = []\n",
        "\n",
        "  # Tokenize words\n",
        "  if TOKENIZATION_ALGORITHM == 1:\n",
        "    tokenized = TreebankWordTokenizer().tokenize(jd_files_dict[key].lower())\n",
        "  else:\n",
        "    tokenized = jd_files_dict[key].lower().split()\n",
        "\n",
        "  if FILTER_STOP_WORDS == 1:\n",
        "    tokenized = [token for token in tokenized if (token not in stop_words and len(token)>2)]\n",
        "\n",
        "  if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "    tokenized = [stemmer.stem(token) for token in tokenized]\n",
        "\n",
        "  # Tokenize ngrams\n",
        "  if NGRAM_COUNT > 1:\n",
        "    # Handle files that are \"empty\", i.e. contains only spaces\n",
        "    if len(tokenized) == 0:\n",
        "      ngram_tokens = []\n",
        "    else:\n",
        "      ngram_tokens = [' '.join(t) for t in ngrams(tokenized, NGRAM_COUNT)]\n",
        "    tokenized += ngram_tokens\n",
        "\n",
        "  if FILTER_NON_ALPHABETIC_TOKENS == 1:\n",
        "    tokenized = filter_alpha_space(tokenized)\n",
        "\n",
        "  jds_tokenized[key] = tokenized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6min 43s (started: 2021-10-27 19:51:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvaK0hoGGKBq",
        "outputId": "0937dd7d-58ec-41e9-d34a-5fd6a1abc53e"
      },
      "source": [
        "with open(JD_TOKENS_PICKLE_OUTPATH, 'wb') as fh:\n",
        "   pickle.dump(jds_tokenized, fh)\n",
        "with open(RESUME_TOKENS_PICKLE_OUTPATH, 'wb') as fh:\n",
        "   pickle.dump(resumes_tokenized, fh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 15.5 s (started: 2021-11-04 18:11:12 +00:00)\n"
          ]
        }
      ]
    }
  ]
}