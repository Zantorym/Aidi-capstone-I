{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDI1003 Capstone Dataset Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDs9RqdgLOEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9db355-ecca-4bc5-fa3f-01ca21637246"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "time: 1.71 ms (started: 2021-10-27 19:48:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2p0FHxT_l31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b072bed4-ac47-43cf-a8be-5f7060de0ec5"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.util import ngrams"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 584 ms (started: 2021-10-27 19:48:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyA4cecCLXBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf09ef5-0ecb-451f-f7d0-79cc2d2ec746"
      },
      "source": [
        "# Constants\n",
        "# Pickle Input\n",
        "JD_FILES_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/JDs/jds.pickle'\n",
        "RESUME_FILES_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/Resumes/resumes.pickle'\n",
        "\n",
        "# Tokenization\n",
        "'''\n",
        "0 - string split\n",
        "1 - NLTK TreebankWordTokenizer\n",
        "Note: not defined value = 0 = string split\n",
        "'''\n",
        "TOKENIZATION_ALGORITHM=0\n",
        "\n",
        "\n",
        "# NGrams\n",
        "NGRAM_COUNT=2\n",
        "\n",
        "# Stop Words\n",
        "FILTER_STOP_WORDS=1\n",
        "STOP_WORDS_SOURCE=0\n",
        "'''\n",
        "1 - Use NLTK stop words\n",
        "2 - Use Scikit Learn stop words\n",
        "Note: not defined value = 0 = intersection of both NLTK and Scikit-learn\n",
        "'''\n",
        "\n",
        "# Case Folding\n",
        "# Note: case folding is always performed as job description and resumes\n",
        "#       should have minimal use of proper nouns for differentiating against \n",
        "#       common words.\n",
        "\n",
        "# Stemming\n",
        "STEMMER_ALGORITHM=2\n",
        "'''\n",
        "1 = Use Porter stemmer\n",
        "2 = Use Snowball stemmer\n",
        "Note: not defined value = 0 = no stemming performed\n",
        "'''\n",
        "\n",
        "# Lemmatization\n",
        "# Note: Cannot perform lematization as punctuation is removed from source text.\n",
        "#       Lemmatization requires parts of speech to work properly.\n",
        "\n",
        "# Pickle Output\n",
        "JD_TOKENS_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/JDs/jds-tokenized.pickle'\n",
        "RESUME_TOKENS_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/Resumes/resumes-tokenized.pickle'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.6 ms (started: 2021-10-27 19:48:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fueE2I5YOM-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78be07de-5b1b-427f-ec99-b0c21f900e4a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 2.44 ms (started: 2021-10-27 19:48:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ9orrgWMRlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf52d82d-8ff0-4e9e-96ac-d895144c8faf"
      },
      "source": [
        "jd_files_dict = resume_files_dict = {}\n",
        "with open(JD_FILES_PICKLE_OUTPATH, 'rb') as fh:\n",
        "  jd_files_dict = pickle.load(fh)\n",
        "with open(RESUME_FILES_PICKLE_OUTPATH, 'rb') as fh:\n",
        "  resume_files_dict = pickle.load(fh)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.3 s (started: 2021-10-27 19:48:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26EwFigkFiD",
        "outputId": "a35d7e54-1f1a-4f83-dbc2-8dcd98e7f57b"
      },
      "source": [
        "if FILTER_STOP_WORDS == 1:\n",
        "  if STOP_WORDS_SOURCE != 2:\n",
        "    nltk.download('stopwords')\n",
        "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
        "  \n",
        "  if STOP_WORDS_SOURCE != 1:\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
        "\n",
        "  if STOP_WORDS_SOURCE == 1:\n",
        "    stop_words = nltk_stop_words\n",
        "  elif STOP_WORDS_SOURCE == 2:\n",
        "    stop_words = sklearn_stop_words\n",
        "  else:\n",
        "    stop_words = sklearn_stop_words.intersection(nltk_stop_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "time: 43.7 ms (started: 2021-10-27 19:49:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH-g6ZJ-5sEH",
        "outputId": "c376250b-904f-423a-9d42-a43dac85a1fb"
      },
      "source": [
        "if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "  from nltk.stem.snowball import SnowballStemmer\n",
        "  if STEMMER_ALGORITHM == 1:\n",
        "    stemmer = SnowballStemmer(language='porter')\n",
        "  elif STEMMER_ALGORITHM == 2:\n",
        "    stemmer = SnowballStemmer(language='english')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.57 ms (started: 2021-10-27 19:49:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aau7f6Wd3HCS",
        "outputId": "f723dd4e-6806-401c-c344-7a59a0bcc75b"
      },
      "source": [
        "# Tokenize Resumes\n",
        "resumes_tokenized = {}\n",
        "for key in resume_files_dict:\n",
        "  tokenized = []\n",
        "\n",
        "  # Tokenize words\n",
        "  if TOKENIZATION_ALGORITHM == 1:\n",
        "    tokenized = TreebankWordTokenizer().tokenize(resume_files_dict[key].lower())\n",
        "  else:\n",
        "    tokenized = resume_files_dict[key].lower().split()\n",
        "\n",
        "  if FILTER_STOP_WORDS == 1:\n",
        "    tokenized = [token for token in tokenized if token not in stop_words]\n",
        "\n",
        "  if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "    tokenized = [stemmer.stem(token) for token in tokenized]\n",
        "\n",
        "  # Tokenize ngrams\n",
        "  if NGRAM_COUNT > 1:\n",
        "    # Handle files that are \"empty\", i.e. contains only spaces\n",
        "    if len(tokenized) == 0:\n",
        "      ngram_tokens = []\n",
        "    else:\n",
        "      ngram_tokens = [' '.join(t) for t in ngrams(tokenized, NGRAM_COUNT)]\n",
        "    tokenized += ngram_tokens\n",
        "\n",
        "  resumes_tokenized[key] = tokenized"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 57s (started: 2021-10-27 19:49:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3axQ4ALnS6N",
        "outputId": "9276fba0-3dfa-4c74-bd18-d99f254245c7"
      },
      "source": [
        "# Tokenize JDs\n",
        "jds_tokenized = {}\n",
        "for key in jd_files_dict:\n",
        "  tokenized = []\n",
        "\n",
        "  # Tokenize words\n",
        "  if TOKENIZATION_ALGORITHM == 1:\n",
        "    tokenized = TreebankWordTokenizer().tokenize(jd_files_dict[key].lower())\n",
        "  else:\n",
        "    tokenized = jd_files_dict[key].lower().split()\n",
        "\n",
        "  if FILTER_STOP_WORDS == 1:\n",
        "    tokenized = [token for token in tokenized if token not in stop_words]\n",
        "\n",
        "  if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "    tokenized = [stemmer.stem(token) for token in tokenized]\n",
        "\n",
        "  # Tokenize ngrams\n",
        "  if NGRAM_COUNT > 1:\n",
        "    # Handle files that are \"empty\", i.e. contains only spaces\n",
        "    if len(tokenized) == 0:\n",
        "      ngram_tokens = []\n",
        "    else:\n",
        "      ngram_tokens = [' '.join(t) for t in ngrams(tokenized, NGRAM_COUNT)]\n",
        "    tokenized += ngram_tokens\n",
        "\n",
        "  jds_tokenized[key] = tokenized"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6min 43s (started: 2021-10-27 19:51:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvaK0hoGGKBq",
        "outputId": "d30d9f28-a7ca-43ea-da93-f9241fa61a6b"
      },
      "source": [
        "with open(JD_TOKENS_PICKLE_OUTPATH, 'wb') as fh:\n",
        "   pickle.dump(jds_tokenized, fh)\n",
        "with open(RESUME_TOKENS_PICKLE_OUTPATH, 'wb') as fh:\n",
        "   pickle.dump(resumes_tokenized, fh)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 49.8 s (started: 2021-10-27 19:58:41 +00:00)\n"
          ]
        }
      ]
    }
  ]
}