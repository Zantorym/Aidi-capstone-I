{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 - Similarity Finder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zantorym/Aidi-capstone-I/blob/review/AIDI1003_Capstone_Dataset_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Setup"
      ],
      "metadata": {
        "id": "YEr2pkHFWgyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install autotime for reporting of how long code runs"
      ],
      "metadata": {
        "id": "JgrQg0COWiSi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDs9RqdgLOEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d368cb-7709-4612-dfcb-03871351db18"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "time: 2.28 ms (started: 2021-12-11 03:12:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries needed by code"
      ],
      "metadata": {
        "id": "IYN8sE9VWltz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2p0FHxT_l31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93187dae-4a3d-49b5-cd08-09e08f9f5504"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 732 ms (started: 2021-12-11 03:12:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define constants for easier configuration of runs across users"
      ],
      "metadata": {
        "id": "Nv9AHKQwWpMR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyA4cecCLXBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6543caed-d235-45a8-a60f-a857bf9ce41d"
      },
      "source": [
        "# Constants\n",
        "# Pickle Input\n",
        "JD_FILES_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/JDs/jds.pickle'\n",
        "RESUME_FILES_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/Resumes/resumes.pickle'\n",
        "\n",
        "# Tokenization\n",
        "'''\n",
        "0 - string split\n",
        "1 - NLTK TreebankWordTokenizer\n",
        "Note: not defined value = 0 = string split\n",
        "'''\n",
        "TOKENIZATION_ALGORITHM=0\n",
        "\n",
        "\n",
        "# NGrams\n",
        "NGRAM_COUNT=2\n",
        "\n",
        "# Stop Words\n",
        "FILTER_STOP_WORDS=1\n",
        "STOP_WORDS_SOURCE=0\n",
        "'''\n",
        "1 - Use NLTK stop words\n",
        "2 - Use Scikit Learn stop words\n",
        "Note: not defined value = 0 = intersection of both NLTK and Scikit-learn\n",
        "'''\n",
        "\n",
        "# Case Folding\n",
        "# Note: case folding is always performed as job description and resumes\n",
        "#       should have minimal use of proper nouns for differentiating against \n",
        "#       common words.\n",
        "\n",
        "# Stemming\n",
        "STEMMER_ALGORITHM=0\n",
        "'''\n",
        "1 = Use Porter stemmer\n",
        "2 = Use Snowball stemmer\n",
        "Note: not defined value = 0 = no stemming performed\n",
        "'''\n",
        "\n",
        "# Lemmatization\n",
        "# Note: Cannot perform lematization as punctuation is removed from source text.\n",
        "#       Lemmatization requires parts of speech to work properly.\n",
        "\n",
        "# Filtering non-alphabetic tokens\n",
        "FILTER_NON_ALPHABETIC_TOKENS = 1\n",
        "'''\n",
        "0 = Don't filter\n",
        "1 = Filter\n",
        "'''\n",
        "\n",
        "# Pickle Output\n",
        "JD_TOKENS_PICKLE_OUTPATH='/content/drive/MyDrive/AIDI1003/JDs/jds-tokenized.pickle'\n",
        "RESUME_TOKENS_PICKLE_OUTPATH='/content/drive/MyDrive/Durham College/Capstone - I/data/Datasets/resumes-tokenized.pickle'\n",
        "\n",
        "# Number of results to show in ranked output\n",
        "NUM_RESULTS_TO_SHOW=20"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18.6 ms (started: 2021-12-11 03:12:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attach google drive to colab instance"
      ],
      "metadata": {
        "id": "m-KOTg1RWtfB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fueE2I5YOM-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bfc140-09da-426b-9e26-0df8b0ba33ee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 844 ms (started: 2021-12-11 03:12:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "QGrISgntX41g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites"
      ],
      "metadata": {
        "id": "5VRV_2TPWypH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset from pickled version output of previous stage"
      ],
      "metadata": {
        "id": "2fOWMlxcW4Dj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ9orrgWMRlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccaf4b6b-3fd5-41e0-d88f-43546b2bb551"
      },
      "source": [
        "jd_files_dict = resume_files_dict = {}\n",
        "with open(JD_FILES_PICKLE_OUTPATH, 'rb') as fh:\n",
        "  jd_files_dict = pickle.load(fh)\n",
        "with open(RESUME_FILES_PICKLE_OUTPATH, 'rb') as fh:\n",
        "  resume_files_dict = pickle.load(fh)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.42 s (started: 2021-12-11 03:12:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the counts of files loaded"
      ],
      "metadata": {
        "id": "aHKx3JIYZahc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Count of JDs:', len(jd_files_dict))\n",
        "print('Count of Resumes:', len(resume_files_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ubj9mMMZYeZ",
        "outputId": "57625007-e60c-4974-e3aa-3d3721777c48"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of JDs: 151210\n",
            "Count of Resumes: 50023\n",
            "time: 1.87 ms (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare stop words"
      ],
      "metadata": {
        "id": "c56PBI0bW3gD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26EwFigkFiD",
        "outputId": "b1c2a340-7997-449f-99c4-4e9e6407a608"
      },
      "source": [
        "if FILTER_STOP_WORDS == 1:\n",
        "  if STOP_WORDS_SOURCE != 2:\n",
        "    nltk.download('stopwords')\n",
        "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
        "  \n",
        "  if STOP_WORDS_SOURCE != 1:\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
        "\n",
        "  if STOP_WORDS_SOURCE == 1:\n",
        "    stop_words = nltk_stop_words\n",
        "  elif STOP_WORDS_SOURCE == 2:\n",
        "    stop_words = sklearn_stop_words\n",
        "  else:\n",
        "    stop_words = sklearn_stop_words.intersection(nltk_stop_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "time: 63.8 ms (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare stemmer algorithm"
      ],
      "metadata": {
        "id": "svurjRuWXTJU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH-g6ZJ-5sEH",
        "outputId": "3110ed89-9206-4bfb-aa83-2c1d15c3ad17"
      },
      "source": [
        "if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "  from nltk.stem.snowball import SnowballStemmer\n",
        "  if STEMMER_ALGORITHM == 1:\n",
        "    stemmer = SnowballStemmer(language='porter')\n",
        "  elif STEMMER_ALGORITHM == 2:\n",
        "    stemmer = SnowballStemmer(language='english')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.03 ms (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Definitions"
      ],
      "metadata": {
        "id": "jE93ZCMga-AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to filter out non-alphabetic strings from a list of strings"
      ],
      "metadata": {
        "id": "F4oWEGiBXY2N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_vpmrw2Fa_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad2a7c7-3cce-4f17-d07f-3584bfc37add"
      },
      "source": [
        "'''\n",
        "Filters out non-alphabetic strings from a list of strings\n",
        "\n",
        "args:\n",
        "  * words - A list of strings\n",
        "\n",
        "Returns:\n",
        "  * A filtered list of strings\n",
        "'''\n",
        "def filter_alpha_space(words):\n",
        "  fil = []\n",
        "  for string in words:\n",
        "    if (any(x.isalpha() for x in string) and all(x.isalpha() or x.isspace() for x in string)):\n",
        "      fil.append(string) \n",
        "  return fil"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.27 ms (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplified tokenization function used in vectorization"
      ],
      "metadata": {
        "id": "Vg1gypZ6YyX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tokenization function used in vectorization\n",
        "\n",
        "args:\n",
        "  * text - the input text to be tokenized\n",
        "\n",
        "returns:\n",
        "  * the tokens that represent the text\n",
        "'''\n",
        "def tokenize(text):\n",
        "  tokenized = text.lower().split()\n",
        "  tokenized = [token for token in tokenized if (len(token)>1 and all(char.isalpha()or char.isdigit() for char in token))]\n",
        "  return tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceWIcr55Yx-m",
        "outputId": "8f16d31a-c344-4444-c93f-fe3e3cf1e84d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.77 ms (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to request file types for processing"
      ],
      "metadata": {
        "id": "x3upNWekZ0PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Gets and checks the type of file for processing\n",
        "Accepted file types:\n",
        "  * 1 - Job Description file (JD)\n",
        "  * 2 - Resume file\n",
        "\n",
        "args:\n",
        "  * field_name - the name of the type being requested\n",
        "\n",
        "returns:\n",
        "  * the file type obtained, can be None if input is not valid\n",
        "'''\n",
        "def get_file_type(field_name: str):\n",
        "  user_input: str = \\\n",
        "    input(\"Please enter the file type for the {} file(s).\\n\".format(field_name))\n",
        "\n",
        "  if user_input == '1' or user_input == '2':\n",
        "    user_input = int(user_input)\n",
        "    print('You have entered \"{}\" as the {} file type.'.format(user_input, \n",
        "                                                              field_name))    \n",
        "    print('Thank you for your input.')\n",
        "  else:\n",
        "    print('You have entered an invalid value of \"{}\" as the {} file type.'\\\n",
        "          .format(user_input, field_name))\n",
        "    user_input = None\n",
        "  return user_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrze6XXGZykP",
        "outputId": "7a0211c2-e9c2-4678-ae94-8d2c9b2346ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.88 ms (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Handling"
      ],
      "metadata": {
        "id": "rXtvLnHva5Xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get input file type"
      ],
      "metadata": {
        "id": "PcWwZrAOaqO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_type = None\n",
        "while input_type is None:\n",
        "  input_type = get_file_type(\"input\")\n",
        "\n",
        "input_dictionary = jd_files_dict if input_type == 1 else resume_files_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NemSfAX4ZiQg",
        "outputId": "220381de-3e3b-4e23-c3cf-4e2da12f33ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the file type for the input file(s).\n",
            "1\n",
            "You have entered \"1\" as the input file type.\n",
            "Thank you for your input.\n",
            "time: 3.75 s (started: 2021-12-11 03:12:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the input filename"
      ],
      "metadata": {
        "id": "CP9yRBr0bD9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_filename = None\n",
        "while input_filename is None:\n",
        "  input_filename = input(\"Please enter the file name for the input file.\\n\")\n",
        "  print('You have entered \"{}\" as the input file name.'.format(input_filename))\n",
        "  if not (input_filename in input_dictionary):\n",
        "    print('The specified filename is not in the input dictionary, try again.')\n",
        "    input_filename = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXmftiAZbFeU",
        "outputId": "8aef0196-a359-4c7b-d24e-a71f39e1e90a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the file name for the input file.\n",
            "ABAP Consuultant_23157\n",
            "You have entered \"ABAP Consuultant_23157\" as the input file name.\n",
            "time: 8.54 s (started: 2021-12-11 03:12:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get output file type"
      ],
      "metadata": {
        "id": "x3PhIbB8bHPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_type = None\n",
        "while output_type is None:\n",
        "  output_type = get_file_type(\"output\")\n",
        "\n",
        "output_dictionary = jd_files_dict if output_type == 1 else resume_files_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPNCsGUtbJAw",
        "outputId": "28c8637d-015f-4101-b318-f19d886086c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the file type for the output file(s).\n",
            "1\n",
            "You have entered \"1\" as the output file type.\n",
            "Thank you for your input.\n",
            "time: 1.27 s (started: 2021-12-11 03:13:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "gteIFWxgXmAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize resume files"
      ],
      "metadata": {
        "id": "mKnJjF_wXpk1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aau7f6Wd3HCS",
        "outputId": "1e0f851a-0713-474f-ce63-cc43d5582196"
      },
      "source": [
        "# Tokenize Resumes\n",
        "resumes_tokenized = {}\n",
        "for key in resume_files_dict:\n",
        "  tokenized = []\n",
        "\n",
        "  # Tokenize words\n",
        "  if TOKENIZATION_ALGORITHM == 1:\n",
        "    tokenized = TreebankWordTokenizer().tokenize(resume_files_dict[key].lower())\n",
        "  else:\n",
        "    tokenized = resume_files_dict[key].lower().split()\n",
        "\n",
        "  if FILTER_STOP_WORDS == 1:\n",
        "    tokenized = [token for token in tokenized if (token not in stop_words and len(token)>2)]\n",
        "\n",
        "  if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "    tokenized = [stemmer.stem(token) for token in tokenized]\n",
        "\n",
        "  # Tokenize ngrams\n",
        "  if NGRAM_COUNT > 1:\n",
        "    # Handle files that are \"empty\", i.e. contains only spaces\n",
        "    if len(tokenized) == 0:\n",
        "      ngram_tokens = []\n",
        "    else:\n",
        "      ngram_tokens = [' '.join(t) for t in ngrams(tokenized, NGRAM_COUNT)]\n",
        "    tokenized += ngram_tokens\n",
        "\n",
        "  if FILTER_NON_ALPHABETIC_TOKENS == 1:\n",
        "    tokenized = filter_alpha_space(tokenized)\n",
        "\n",
        "  resumes_tokenized[key] = tokenized"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1min (started: 2021-12-11 03:13:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize job description files"
      ],
      "metadata": {
        "id": "DD1O_q5_XtzU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3axQ4ALnS6N",
        "outputId": "95b495d6-52f9-4bcd-8697-1d15ea6646f2"
      },
      "source": [
        "# Tokenize JDs\n",
        "jds_tokenized = {}\n",
        "for key in jd_files_dict:\n",
        "  tokenized = []\n",
        "\n",
        "  # Tokenize words\n",
        "  if TOKENIZATION_ALGORITHM == 1:\n",
        "    tokenized = TreebankWordTokenizer().tokenize(jd_files_dict[key].lower())\n",
        "  else:\n",
        "    tokenized = jd_files_dict[key].lower().split()\n",
        "\n",
        "  if FILTER_STOP_WORDS == 1:\n",
        "    tokenized = [token for token in tokenized if (token not in stop_words and len(token)>2)]\n",
        "\n",
        "  if STEMMER_ALGORITHM == 1 or STEMMER_ALGORITHM == 2:\n",
        "    tokenized = [stemmer.stem(token) for token in tokenized]\n",
        "\n",
        "  # Tokenize ngrams\n",
        "  if NGRAM_COUNT > 1:\n",
        "    # Handle files that are \"empty\", i.e. contains only spaces\n",
        "    if len(tokenized) == 0:\n",
        "      ngram_tokens = []\n",
        "    else:\n",
        "      ngram_tokens = [' '.join(t) for t in ngrams(tokenized, NGRAM_COUNT)]\n",
        "    tokenized += ngram_tokens\n",
        "\n",
        "  if FILTER_NON_ALPHABETIC_TOKENS == 1:\n",
        "    tokenized = filter_alpha_space(tokenized)\n",
        "\n",
        "  jds_tokenized[key] = tokenized"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 24s (started: 2021-12-11 03:14:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: due to long processing times the manually tokenized versions above are not used in vectorization. Instead pre-existing libraries are used as they are optimized for processing speed."
      ],
      "metadata": {
        "id": "lHpNhmNTdZpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization"
      ],
      "metadata": {
        "id": "vfSbMZTbZfHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add an entry for the input text into the output dictionary before vectorization"
      ],
      "metadata": {
        "id": "BaCVRXjhaswv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_key = 'input:'+input_filename\n",
        "output_dictionary[input_key] = input_dictionary[input_filename]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaubIVYybUHw",
        "outputId": "1cd68488-7521-45bf-9cb6-567506749910"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.05 ms (started: 2021-12-11 03:16:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare corpus"
      ],
      "metadata": {
        "id": "SYECpDngbbTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_raw = pd.DataFrame.from_dict(output_dictionary, orient='index', columns=['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez-5uF2ubaf1",
        "outputId": "5648a7b3-0294-43b5-8aaf-70d480a5dbb3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 109 ms (started: 2021-12-11 03:16:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save filenames for numerical index retrieval later"
      ],
      "metadata": {
        "id": "CQkpeuKHbfkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_filenames = corpus_raw.index.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxy5LIZYbeR4",
        "outputId": "2a9a53f2-2047-4bfa-ffb0-258525421be1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.91 ms (started: 2021-12-11 03:16:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize"
      ],
      "metadata": {
        "id": "2phYlvTEbhT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words, ngram_range=(1, 2))\n",
        "corpus_vectors = vectorizer.fit_transform(corpus_raw['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb8k8y60blLX",
        "outputId": "be8a8dbe-1249-4efe-a70b-89009e50d5b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 23s (started: 2021-12-11 03:16:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get vector representing input"
      ],
      "metadata": {
        "id": "cjA8BZA3bmla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_index = np.where(corpus_filenames == input_key)[0][0]\n",
        "input_vector = corpus_vectors[input_index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Q1PqxYbojJ",
        "outputId": "e0719ccb-4758-4fa9-fbe4-13447d880e89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.52 ms (started: 2021-12-11 03:18:53 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity"
      ],
      "metadata": {
        "id": "lbetVa13brO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Cosine Similarity"
      ],
      "metadata": {
        "id": "Zqh9wwXxbu6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarity_output = cosine_similarity(corpus_vectors, input_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J6X1wuLbw6x",
        "outputId": "89841723-3219-4f0d-f0dc-c614dd3b1f25"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.06 s (started: 2021-12-11 03:18:53 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the most similar results"
      ],
      "metadata": {
        "id": "3CIuawS1b0Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarity_df = pd.DataFrame(cos_similarity_output, index = \\\n",
        "                                 corpus_filenames, columns = ['similarity'])\n",
        "drop_indices = [input_key]\n",
        "if input_type == output_type:\n",
        "  drop_indices.append(input_filename)\n",
        "\n",
        "cos_similarity_df.drop(index = drop_indices).nlargest(NUM_RESULTS_TO_SHOW,\n",
        "                                                          'similarity')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "LDdYQC09b4Qh",
        "outputId": "69c14054-cf72-4760-bdce-d2b74777d44c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Contract Administr_23653</th>\n",
              "      <td>0.163727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C Developer_28577</th>\n",
              "      <td>0.111950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sr Java Developer_23658</th>\n",
              "      <td>0.105989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP ABAP Consultan_58429</th>\n",
              "      <td>0.094648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HCL tech is Lookin_60402</th>\n",
              "      <td>0.091055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Opening for Positi_60744</th>\n",
              "      <td>0.083848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP ABAP Consultan_58075</th>\n",
              "      <td>0.074864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP SRM Consultant_67527</th>\n",
              "      <td>0.074740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Application Develo_33324</th>\n",
              "      <td>0.071890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sap ABAP HANA Open_57494</th>\n",
              "      <td>0.066970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP ABAP APO Consu_22114</th>\n",
              "      <td>0.065310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Data entry_36335</th>\n",
              "      <td>0.065156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP SD Consultant_66540</th>\n",
              "      <td>0.063332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP ABAP FI Consul_64136</th>\n",
              "      <td>0.062847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ABAP Consultant lo_71384</th>\n",
              "      <td>0.062042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAP IDOC with EDI_77124</th>\n",
              "      <td>0.061099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Assistant to Direc_8171</th>\n",
              "      <td>0.060188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Software Engineer_4526</th>\n",
              "      <td>0.059850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hiringlevel Comp_65368</th>\n",
              "      <td>0.057327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Manual Test Lead_34665</th>\n",
              "      <td>0.056698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          similarity\n",
              "Contract Administr_23653    0.163727\n",
              "C Developer_28577           0.111950\n",
              "Sr Java Developer_23658     0.105989\n",
              "SAP ABAP Consultan_58429    0.094648\n",
              "HCL tech is Lookin_60402    0.091055\n",
              "Opening for Positi_60744    0.083848\n",
              "SAP ABAP Consultan_58075    0.074864\n",
              "SAP SRM Consultant_67527    0.074740\n",
              "Application Develo_33324    0.071890\n",
              "Sap ABAP HANA Open_57494    0.066970\n",
              "SAP ABAP APO Consu_22114    0.065310\n",
              "Data entry_36335            0.065156\n",
              "SAP SD Consultant_66540     0.063332\n",
              "SAP ABAP FI Consul_64136    0.062847\n",
              "ABAP Consultant lo_71384    0.062042\n",
              "SAP IDOC with EDI_77124     0.061099\n",
              "Assistant to Direc_8171     0.060188\n",
              "Software Engineer_4526      0.059850\n",
              "Hiringlevel Comp_65368      0.057327\n",
              "Manual Test Lead_34665      0.056698"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 97.1 ms (started: 2021-12-11 03:18:54 +00:00)\n"
          ]
        }
      ]
    }
  ]
}